# ğŸš¦ Smart Traffic Light Control using Q-Learning (Jupyter Notebook)

This project demonstrates a simplified simulation of a smart traffic light system powered by **Reinforcement Learning (Q-Learning)**. The goal is to reduce congestion at a single intersection by making intelligent decisions about when to switch traffic phases based on real-time queue data and incidents.

---

## ğŸ“š Project Overview

The system simulates:
- **TrafficLightAgent**: Learns to switch signals using Q-learning to minimize queue congestion.
- **VehicleAgent**: Represents vehicles moving north-south (NS) or east-west (EW), including emergency vehicles.
- **DroneAgent**: Randomly detects incidents like accidents that block routes temporarily.
- **TollAgent**: Adjusts toll pricing dynamically based on congestion.

The simulation runs in discrete steps, with the environment updating states, collecting rewards, and visualizing system performance.

---

## ğŸ§  Features

- ğŸ” **Q-Learning Agent** that learns optimal switching strategy
- ğŸš“ **Emergency Vehicle Handling** to ensure priority
- ğŸš **Incident Detection** using Drone agent
- ğŸ’¸ **Dynamic Toll Pricing** based on traffic load
- ğŸ“Š **Visualization** of queue lengths, tolls, rewards, and cleared vehicles
- ğŸ¤– Optional: Deep Q-Network (DQN) agent for performance comparison

---

---

## âš™ï¸ Setup Instructions

### 1. Install Dependencies

You can install required packages using pip:

```bash
pip install -r requirements.txt

```
requirement.txt

numpy
matplotlib
torch
notebook


## How to Run the Simulation

Open smart_traffic_light.ipynb.

Run all cells (Kernel > Restart & Run All).

Observe printed logs and visual plots for performance evaluation.


## Visual Outputs
At the end of the simulation, the notebook produces:

ğŸš¦ Queue Length Over Time (NS, EW, Total)

ğŸ’¸ Toll Pricing Over Time

ğŸ“ˆ Cumulative Reward Plot

ğŸš— Vehicles Cleared Over Time

ğŸ§  Top Q-values Learned from training

## ğŸ§ª How to Test
You can test different configurations by modifying parameters:

In the TrafficEnvironment:

num_vehicles: Total number of vehicles

emergency_vehicle_probability: Chance of emergency vehicles

In the TrafficLightAgent:

alpha: Learning rate

gamma: Discount factor

epsilon: Exploration rate

After changes, re-run the cells to see updated behaviors and performance metrics.

## ğŸš€ Future Enhancements
ğŸ¤– Implement and compare Deep Q-Network (DQN) performance

ğŸ™ï¸ Simulate a grid of multiple intersections

ğŸ›°ï¸ Use real-time traffic data inputs

ğŸ“± Develop a UI using Streamlit or Voila

ğŸ§ª Train over multiple episodes to improve learning stability

![image](https://github.com/user-attachments/assets/379afc96-0d3a-485d-9ef3-1e6fccd1c288)

![image](https://github.com/user-attachments/assets/1a47fc8e-865f-4b1f-90c7-c2dcc5c1dadd)


